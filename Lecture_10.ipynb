{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI_4170_6170_Spring2026/blob/main/Lecture_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB5wIGt93JNj"
      },
      "source": [
        "# Lecture 10 — Natural Language Processing and Language Models\n",
        "\n",
        "## Agenda\n",
        "1. What is NLP? tasks, datasets, evaluation\n",
        "2. Text preprocessing and tokenization\n",
        "3. Vocabularies and embeddings\n",
        "4. Language modeling (probabilities, objectives, metrics)\n",
        "5. Seq2Seq + attention\n",
        "6. Transformers and modern LLMs\n",
        "\n",
        "## Learning objectives\n",
        "By the end of this lecture you should be able to:\n",
        "- Explain the standard NLP pipeline and where neural models fit.\n",
        "- Compare word-, character-, and subword-tokenization and choose between them.\n",
        "- Write the language-model objective using the chain rule and connect it to cross-entropy loss.\n",
        "- Explain why full softmax is expensive and how sampled-softmax / NCE approximate it.\n",
        "- Describe encoder–decoder (seq2seq) training vs inference, and why attention helps.\n",
        "- Identify the key components of a Transformer (self-attention, multi-head, positional encoding, masking).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wATsjgao3elW"
      },
      "source": [
        "## 1) Introduction\n",
        "\n",
        "**Natural Language Processing (NLP)** covers any computational method for working with human language (text or speech).  \n",
        "Typical applications include translation, sentiment analysis, information extraction (NER), summarization, speech recognition, and text generation (chatbots / LLMs).\n",
        "\n",
        "### Rule-based vs. ML-based vs. Foundation-model approaches\n",
        "- **Rule-based / symbolic**: regex, grammars, dictionaries, deterministic pipelines.\n",
        "- **Classical ML**: engineered features (n-grams, TF-IDF) + linear models (logistic regression, SVM).\n",
        "- **Neural / deep learning**: learned representations (embeddings) + sequence models (RNN/LSTM/GRU, CNNs, Transformers).\n",
        "- **Foundation models (LLMs)**: large Transformer models pretrained on broad text corpora; adapted via fine-tuning or prompting.\n",
        "\n",
        "> Important: search engines and IR systems often rely on non-ML algorithms (indexing + retrieval) plus **optional** ML ranking models. Many “NLP products” are hybrid systems.\n",
        "\n",
        "### Common NLP pipeline (high level)\n",
        "1. **Collect data**: text corpus, labels (if supervised), train/val/test splits.\n",
        "2. **Preprocess**: normalization (lowercasing, punctuation handling), tokenization, filtering.\n",
        "3. **Represent text**: tokens → ids → embeddings.\n",
        "4. **Model**: classifier / tagger / seq2seq model / language model.\n",
        "5. **Train & evaluate**: metrics depend on task (accuracy/F1, BLEU/ROUGE, perplexity, etc.).\n",
        "6. **Deploy**: latency, memory, safety constraints, monitoring for drift.\n",
        "\n",
        "### Key NLP tasks (examples)\n",
        "- **Tokenization**, **stemming** (reduce to a stem), **lemmatization** (map to dictionary form).\n",
        "- **Part-of-speech tagging**, **named-entity recognition (NER)**.\n",
        "- **Syntactic parsing** and **semantic analysis**.\n",
        "- **Text classification** (sentiment, topic), **retrieval/ranking**, **generation**.\n",
        "\n",
        "### Challenges in NLP\n",
        "- Ambiguity and polysemy (same word, different meaning).\n",
        "- Context dependence, pragmatics, sarcasm/irony.\n",
        "- Domain shift (train on news, deploy on social media).\n",
        "- Multilingual & low-resource settings.\n",
        "- Long-range dependencies in sequences (motivation for attention/Transformers).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBp-C5wV3_Wf"
      },
      "source": [
        "## 2) Vocabulary, tokens, and embeddings\n",
        "\n",
        "Most language models and NLP pipelines start by converting raw text into **tokens**, then into **integer ids** from a **vocabulary**.\n",
        "\n",
        "### Key terms\n",
        "1. **Text corpus**: the collection of documents/sentences used for training/evaluation.\n",
        "2. **Vocabulary**: the set of tokens the model can represent.\n",
        "   - Usually includes **special tokens** such as: `[PAD]`, `[UNK]`/`OOV`, `[BOS]`/`<s>`, `[EOS]`/`</s>`.\n",
        "   - Often built by frequency cutoff (keep top-*V* tokens) to control memory/compute.\n",
        "3. **Tokenization**: mapping text → tokens. Common types:\n",
        "   - **Word tokenization**: split into words (works well when word boundaries are clear).\n",
        "   - **Character tokenization**: split into characters (handles OOV but sequences become long).\n",
        "   - **Subword tokenization** (BPE, WordPiece, SentencePiece): split words into frequent pieces.\n",
        "     - Example: “unbelievable” → `un`, `believ`, `able` (varies by tokenizer).\n",
        "     - Helps with rare words and morphology; reduces OOV rate.\n",
        "4. **Embeddings**: learned dense vectors for tokens.\n",
        "   - A vocabulary of size *V* and embedding dimension *d* gives an embedding matrix **E ∈ R^{V×d}**.\n",
        "   - Tokens become vectors via table lookup (much smaller than one-hot vectors).\n",
        "   - Classic: Word2Vec, GloVe; modern: embeddings are usually learned jointly with the downstream model.\n",
        "\n",
        "### Why subwords are dominant in modern LLMs\n",
        "- Word vocabularies explode in size (names, typos, domain terms).\n",
        "- Character models are robust but slower (longer sequences).\n",
        "- Subwords are a strong trade-off: smaller vocab than words, shorter sequences than characters.\n",
        "\n",
        "### Candidate sampling (why we care)\n",
        "Training many NLP models (embeddings, language models) involves predicting a token among **V** possibilities.  \n",
        "A naive softmax is **O(V)** per prediction step → expensive when V is large (e.g., 30k–200k+).\n",
        "\n",
        "Approximations such as **sampled softmax**, **NCE**, or **negative sampling** reduce compute by only comparing against a small set of sampled “negative” tokens each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GVwOKAAvq7T",
        "outputId": "f62b5e22-1d3e-4870-a26b-01f01ed251b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (including [OOV]): 14\n",
            "Word index (token -> id):\n",
            "{'[OOV]': 1, 'a': 2, 'bob': 3, 'e': 4, 'apples': 5, 'd': 6, 'pears': 7, 'fred': 8, 'did': 9, 'o': 10, 'ea': 11, 'ba': 12, 'as': 13}\n",
            "\n",
            "New texts -> sequences:\n",
            "'Bob ate fruit' -> [3, 2, 4, 1]\n",
            "'Fred ate pears' -> [8, 2, 4, 7]\n",
            "'Alice ate apples' -> [1, 2, 4, 5]\n",
            "\n",
            "Padded sequences (post-padding to length 6 ):\n",
            "[[3 2 4 1 0 0]\n",
            " [8 2 4 7 0 0]\n",
            " [1 2 4 5 0 0]]\n",
            "\n",
            "Decode the first padded sequence:\n",
            "['bob', 'a', 'e', '[OOV]', '[PAD]', '[PAD]']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# A tiny corpus (in practice you would have thousands/millions of sentences)\n",
        "text_corpus = [\n",
        "    \"Bob ate apples, and pears.\",\n",
        "    \"Fred ate apples!\",\n",
        "    \"Bob did not eat bananas.\"\n",
        "]\n",
        "\n",
        "# Tokenizer maps tokens -> integer ids. oov_token catches unseen tokens at inference time.\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    oov_token=\"[OOV]\",\n",
        "    filters=r'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'  # default filters; customize as needed\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(text_corpus)\n",
        "\n",
        "print(\"Vocabulary size (including [OOV]):\", len(tokenizer.word_index) + 1)  # +1 because index starts at 1\n",
        "print(\"Word index (token -> id):\")\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "# Convert new texts to sequences of ids\n",
        "new_texts = [\"Bob ate fruit\", \"Fred ate pears\", \"Alice ate apples\"]\n",
        "seqs = tokenizer.texts_to_sequences(new_texts)\n",
        "print(\"\\nNew texts -> sequences:\")\n",
        "for t, s in zip(new_texts, seqs):\n",
        "    print(f\"{t!r} -> {s}\")\n",
        "\n",
        "# Padding: neural nets typically want fixed-length sequences\n",
        "max_len = 6\n",
        "padded = pad_sequences(seqs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "print(\"\\nPadded sequences (post-padding to length\", max_len, \"):\")\n",
        "print(padded)\n",
        "\n",
        "# Reverse lookup (id -> token) is useful for debugging\n",
        "id_to_token = {idx: tok for tok, idx in tokenizer.word_index.items()}\n",
        "id_to_token[0] = \"[PAD]\"\n",
        "print(\"\\nDecode the first padded sequence:\")\n",
        "print([id_to_token[i] for i in padded[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60fWJX7mM5Yd"
      },
      "source": [
        "## 3) Word probabilities and the language-model objective\n",
        "\n",
        "A **language model (LM)** assigns a probability to a sequence of tokens:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_T)\n",
        "$$\n",
        "\n",
        "Using the **chain rule**:\n",
        "\n",
        "$$\n",
        "P(w_1, \\dots, w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_{<t})\n",
        "$$\n",
        "\n",
        "So the modeling problem becomes: **predict the next token** given the prefix (context).\n",
        "\n",
        "### Training pairs (next-token prediction)\n",
        "Example sentence:\n",
        "`[\"She\", \"went\", \"to\", \"buy\", \"a\", \"book\"]`\n",
        "\n",
        "We can create input → target pairs such as:\n",
        "- Input: `[\"She\"]` → Target: `\"went\"`\n",
        "- Input: `[\"She\",\"went\"]` → Target: `\"to\"`\n",
        "- ...\n",
        "- Input: `[\"She\",\"went\",\"to\",\"buy\",\"a\"]` → Target: `\"book\"`\n",
        "\n",
        "In practice we:\n",
        "- Choose a **maximum context length** (sequence length) and create sliding windows.\n",
        "- Use **padding** and **masking** so the model ignores `[PAD]` tokens.\n",
        "- Train with teacher forcing (the ground-truth previous tokens are fed in during training).\n",
        "\n",
        "### Evaluation metric: perplexity\n",
        "Perplexity is a common LM metric:\n",
        "\n",
        "$$\n",
        "\\text{PPL} = \\exp\\left(\\frac{1}{N} \\sum_{t=1}^{N} -\\log P(w_t \\mid w_{<t})\\right)\n",
        "$$\n",
        "\n",
        "Lower perplexity ⇒ the model assigns higher probability to the true tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRNHuY4AyHZd"
      },
      "source": [
        "### Mini-example: a simple n-gram language model\n",
        "\n",
        "Before neural LMs, **n-gram models** approximated the context by only looking at the previous *(n−1)* tokens:\n",
        "\n",
        "$$\n",
        "P(w_t \\mid w_{<t}) \\approx P(w_t \\mid w_{t-(n-1)}, \\dots, w_{t-1})\n",
        "$$\n",
        "\n",
        "They are easy to implement, but struggle with:\n",
        "- data sparsity (many rare contexts),\n",
        "- long-range dependencies,\n",
        "- generalization to unseen phrases (unless heavily smoothed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aAhmpHqnyHZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbdc404-6bf7-42a6-a2a6-902c16ff6737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P('the' | '<s>') = 0.4118\n",
            "P('cat' | 'the') = 0.2174\n",
            "P('sat' | 'cat') = 0.2000\n",
            "P('on' | 'sat') = 0.3333\n",
            "P('the' | 'on') = 0.3333\n",
            "P('rug' | 'the') = 0.1304\n",
            "P('</s>' | 'rug') = 0.2308\n",
            "\n",
            "Sentence: the cat sat on the rug\n",
            "Perplexity (bigram LM): 4.011\n"
          ]
        }
      ],
      "source": [
        "# A tiny bigram LM (n=2) with add-k smoothing\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "\n",
        "corpus = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the cat ate the fish\",\n",
        "    \"the dog sat on the rug\",\n",
        "]\n",
        "\n",
        "def tokenize(s):\n",
        "    return [\"<s>\"] + s.lower().split() + [\"</s>\"]\n",
        "\n",
        "bigrams = Counter()\n",
        "unigrams = Counter()\n",
        "\n",
        "for sent in corpus:\n",
        "    toks = tokenize(sent)\n",
        "    unigrams.update(toks)\n",
        "    bigrams.update(zip(toks[:-1], toks[1:]))\n",
        "\n",
        "V = len(unigrams)\n",
        "k = 0.5  # smoothing strength\n",
        "\n",
        "def p_bigram(w_prev, w):\n",
        "    # add-k smoothing: (count(prev,w)+k)/(count(prev)+k*V)\n",
        "    return (bigrams[(w_prev,w)] + k) / (unigrams[w_prev] + k*V)\n",
        "\n",
        "test_sent = \"the cat sat on the rug\"\n",
        "toks = tokenize(test_sent)\n",
        "\n",
        "logp = 0.0\n",
        "for a,b in zip(toks[:-1], toks[1:]):\n",
        "    prob = p_bigram(a,b)\n",
        "    logp += math.log(prob)\n",
        "    print(f\"P({b!r} | {a!r}) = {prob:.4f}\")\n",
        "\n",
        "ppl = math.exp(-logp / (len(toks)-1))\n",
        "print(\"\\nSentence:\", test_sent)\n",
        "print(\"Perplexity (bigram LM):\", round(ppl, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0bEkJXaJh6n"
      },
      "source": [
        "## 4) Loss functions and scalable training\n",
        "\n",
        "### Cross-entropy / negative log-likelihood (NLL)\n",
        "For next-token prediction, the standard objective is to **maximize log-likelihood** (or minimize NLL):\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_{t} \\log P\\big(w_t \\mid w_{<t}\\big)\n",
        "$$\n",
        "\n",
        "With a softmax over a vocabulary of size *V*:\n",
        "\n",
        "$$\n",
        "P(w_t=j \\mid \\cdot) = \\frac{\\exp(z_j)}{\\sum_{i=1}^{V} \\exp(z_i)}\n",
        "$$\n",
        "\n",
        "Computing the denominator is expensive when *V* is large.\n",
        "\n",
        "### 1) Sampled softmax loss\n",
        "Instead of normalizing over all *V* tokens, we sample **K negatives** and compute an approximate softmax.  \n",
        "This reduces compute from **O(V)** to about **O(K)** per step (K ≪ V).\n",
        "\n",
        "TensorFlow: `tf.nn.sampled_softmax_loss`  \n",
        "https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss\n",
        "\n",
        "### 2) Noise Contrastive Estimation (NCE)\n",
        "Turns multiclass prediction into a **binary classification** problem: distinguish true data samples from noise samples.\n",
        "\n",
        "TensorFlow: `tf.nn.nce_loss`  \n",
        "https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss\n",
        "\n",
        "### Practical notes\n",
        "- Candidate sampling works best when the sampled negatives are informative (not all ultra-rare).\n",
        "- For small vocabularies, full softmax can be fine and simpler.\n",
        "- In modern Transformer training, vocabulary sizes are often manageable (e.g., 30k–100k) but the sequence length and model size dominate compute.\n",
        "\n",
        "Other important concepts when working with embeddings:\n",
        "- **Cosine similarity**\n",
        "- **Nearest neighbors / kNN** retrieval\n",
        "- Analogy structure (classic Word2Vec behavior)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcf8AwUtItqR"
      },
      "source": [
        "## 5) Language models (architectures)\n",
        "\n",
        "A **language model** predicts the next token (or fills masked tokens) using a learned representation of context.\n",
        "\n",
        "### Classic neural LMs\n",
        "- **RNN**: maintains a hidden state that updates each time step.\n",
        "- **LSTM / GRU**: gated RNNs that mitigate vanishing gradients, better for longer sequences.\n",
        "- **CNN-based** sequence models (less common now for LMs).\n",
        "\n",
        "### Transformer LMs\n",
        "Transformers replace recurrence with **self-attention**, enabling:\n",
        "- parallel training across tokens,\n",
        "- better modeling of long-range dependencies (in practice),\n",
        "- scaling to very large models/datasets.\n",
        "\n",
        "### Output layer\n",
        "Most LMs end with a projection from hidden state to vocabulary logits, then softmax (or approximations).\n",
        "\n",
        "Useful reference:\n",
        "- SOTA trackers for language modeling: https://paperswithcode.com/task/language-modelling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8vY3yW3yHZg"
      },
      "source": [
        "### Mini-example: train a tiny LSTM language model (toy)\n",
        "\n",
        "This is **not** meant to be state-of-the-art—just to connect:\n",
        "tokenization → dataset windows → Embedding/LSTM → softmax → perplexity.\n",
        "\n",
        "On real corpora you would:\n",
        "- use subword tokenization,\n",
        "- larger models,\n",
        "- regularization,\n",
        "- GPUs/TPUs,\n",
        "- careful train/val/test splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_7abj2rLyHZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f778d2-41a6-417e-d6a0-e54516486a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training loss: 3.021317481994629\n",
            "Approx training perplexity: 20.464\n",
            "\n",
            "Generated (greedy) from prompt 'attention helps':\n",
            "attention helps models models token token token token token models\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "toy_text = \"\"\"we study natural language processing\n",
        "language models predict the next token\n",
        "attention helps models focus on relevant context\n",
        "transformers scale efficiently\n",
        "\"\"\".strip().splitlines()\n",
        "\n",
        "# Build tokenization\n",
        "tok = tf.keras.preprocessing.text.Tokenizer(oov_token='[OOV]')\n",
        "tok.fit_on_texts(toy_text)\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "\n",
        "# Create training sequences: predict next word from previous words (fixed window)\n",
        "seq_len = 5  # context length\n",
        "sequences = []\n",
        "for line in toy_text:\n",
        "    ids = tok.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(ids)):\n",
        "        start = max(0, i - seq_len)\n",
        "        context = ids[start:i]\n",
        "        target = ids[i]\n",
        "        # left-pad context to fixed length\n",
        "        context = [0]*(seq_len - len(context)) + context\n",
        "        sequences.append((context, target))\n",
        "\n",
        "X = np.array([c for c,_ in sequences], dtype=np.int32)\n",
        "y = np.array([t for _,t in sequences], dtype=np.int32)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X,y)).shuffle(256).batch(16)\n",
        "\n",
        "# Define a simple LM\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(vocab_size, 32, mask_zero=True),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dense(vocab_size)  # logits\n",
        "])\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=\"adam\", loss=loss_fn)\n",
        "\n",
        "history = model.fit(dataset, epochs=10, verbose=0)\n",
        "print(\"Final training loss:\", history.history[\"loss\"][-1])\n",
        "\n",
        "# Compute perplexity on the training set (toy)\n",
        "logits = model.predict(X, verbose=0)\n",
        "nll = loss_fn(y, logits).numpy()\n",
        "ppl = float(np.exp(nll))\n",
        "print(\"Approx training perplexity:\", round(ppl, 3))\n",
        "\n",
        "# Greedy next-word generation from a prompt\n",
        "id_to_word = {i:w for w,i in tok.word_index.items()}\n",
        "id_to_word[0] = \"[PAD]\"\n",
        "\n",
        "def generate(prompt, steps=8):\n",
        "    context = tok.texts_to_sequences([prompt])[0]\n",
        "    for _ in range(steps):\n",
        "        ctx = context[-seq_len:]\n",
        "        ctx = [0]*(seq_len-len(ctx)) + ctx\n",
        "        pred = model.predict(np.array([ctx]), verbose=0)[0]\n",
        "        next_id = int(np.argmax(pred))\n",
        "        context.append(next_id)\n",
        "    words = [id_to_word.get(i,'[OOV]') for i in context]\n",
        "    return \" \".join(words)\n",
        "\n",
        "print(\"\\nGenerated (greedy) from prompt 'attention helps':\")\n",
        "print(generate(\"attention helps\", steps=8))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYFGeAZUu21G"
      },
      "source": [
        "## 6) Seq2Seq (Encoder–Decoder) framework\n",
        "\n",
        "**Sequence-to-sequence (seq2seq)** models map an input sequence to an output sequence:\n",
        "- machine translation (English → French),\n",
        "- summarization (document → summary),\n",
        "- dialogue (user message → response).\n",
        "\n",
        "### Training\n",
        "1. **Encoder** consumes the input tokens and produces a representation (hidden states).\n",
        "2. **Decoder** predicts the output tokens one-by-one, conditioned on the encoder output.\n",
        "\n",
        "Important tokens:\n",
        "- **SOS/BOS** (start-of-sequence)\n",
        "- **EOS** (end-of-sequence)\n",
        "\n",
        "### Teacher forcing\n",
        "During training, the decoder typically receives the **ground-truth previous token** as input at each step (teacher forcing).  \n",
        "At inference time, the decoder must use its **own** previously generated tokens.\n",
        "\n",
        "### Inference strategies\n",
        "- **Greedy decoding**: choose argmax at each step (fast, can be suboptimal).\n",
        "- **Beam search**: keep top-*k* partial hypotheses (better quality, slower).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyzqDc1MHLHb"
      },
      "source": [
        "### Encoder/decoder states (LSTM/BiLSTM recap)\n",
        "\n",
        "For an LSTM, the state is usually a pair:\n",
        "- **h**: hidden state\n",
        "- **c**: cell state\n",
        "\n",
        "In TensorFlow/Keras, an LSTM layer can return:\n",
        "- outputs over all time steps,\n",
        "- final states `(h_T, c_T)`.\n",
        "\n",
        "For a **BiLSTM**, you have:\n",
        "- forward final states `(h_f, c_f)`,\n",
        "- backward final states `(h_b, c_b)`.\n",
        "\n",
        "To initialize a **unidirectional** decoder from a BiLSTM encoder, you need to **combine** these:\n",
        "- concatenate: `h0 = [h_f ; h_b]` (then project if needed),\n",
        "- or sum/average (less common),\n",
        "- or use attention over all encoder outputs (common in modern seq2seq).\n",
        "\n",
        "Key idea: the decoder needs a compact summary of the input **and/or** access to all encoder time-step outputs (attention).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lLsFXfKJTbA"
      },
      "source": [
        "## 7) Encoder–Decoder architecture (why it works)\n",
        "\n",
        "The decoder uses the encoder’s representation so that generation is **conditioned** on the input.\n",
        "\n",
        "### Single-layer intuition\n",
        "- Encoder reads input sequence → final state summarizes input.\n",
        "- Decoder starts from that final state → generates output sequence.\n",
        "\n",
        "### Multi-layer case\n",
        "If both encoder and decoder have multiple layers, it is common to:\n",
        "- pass the final state from encoder layer *ℓ* to decoder layer *ℓ* as its initial state.\n",
        "\n",
        "### Limitation\n",
        "A single final state can be a bottleneck for long inputs.  \n",
        "This motivates **attention**, which allows the decoder to consult *all* encoder states.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ2UqD9kMvss"
      },
      "source": [
        "## 8) Attention mechanisms\n",
        "\n",
        "Long-term dependencies are hard to capture using only the final encoder state.  \n",
        "**Attention** allows the decoder to compute a **context vector** as a weighted combination of encoder outputs.\n",
        "\n",
        "### Core idea (decoder attention over encoder outputs)\n",
        "At decoder time step *t*:\n",
        "1. Compute alignment scores between decoder state **s_t** and each encoder output **h_i**.\n",
        "2. Normalize scores with softmax → attention weights **α_{t,i}**.\n",
        "3. Compute context vector:\n",
        "$$\n",
        "c_t = \\sum_i \\alpha_{t,i} h_i\n",
        "$$\n",
        "4. Use **(s_t, c_t)** to predict the next token.\n",
        "\n",
        "### Bahdanau vs. Luong (two classic variants)\n",
        "- **Bahdanau attention** (additive): score uses a small neural net over `[s_t ; h_i]`.\n",
        "- **Luong attention** (multiplicative): score uses dot product / bilinear form.\n",
        "\n",
        "TensorFlow tutorial (NMT with attention):\n",
        "https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "\n",
        "Original attention paper (Bahdanau et al., 2014):\n",
        "https://arxiv.org/pdf/1409.0473.pdf\n",
        "\n",
        "### From encoder–decoder attention → self-attention\n",
        "Transformers generalize attention by letting **every token attend to every other token** (self-attention), enabling rich contextual representations without recurrence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uViYv-CwtKlR"
      },
      "source": [
        "## 9) Rise of Transformers and foundation models\n",
        "\n",
        "A **foundation model** is typically a large Transformer pretrained on broad data.  \n",
        "After pretraining, the same base model can be adapted to many tasks via:\n",
        "- fine-tuning (supervised),\n",
        "- instruction tuning,\n",
        "- RLHF / preference optimization,\n",
        "- or prompting (in-context learning).\n",
        "\n",
        "Key properties that made Transformers dominant:\n",
        "- parallelizable training (no recurrence),\n",
        "- strong scaling behavior (more data/params → better performance),\n",
        "- flexible architectures: encoder-only (BERT), decoder-only (GPT), encoder–decoder (T5).\n",
        "\n",
        "Survey-style reference:\n",
        "https://arxiv.org/abs/2108.07258\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Dgw8c6R_TJ"
      },
      "source": [
        "## 10) Transformers (high-level anatomy)\n",
        "\n",
        "Transformers were introduced in *“Attention Is All You Need”* (Vaswani et al., 2017).  \n",
        "https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "### Key components\n",
        "1. **Token embeddings** + **positional encodings** (order information).\n",
        "2. **Self-attention** (often multi-head):\n",
        "   - each head attends with a different learned projection,\n",
        "   - outputs are concatenated and projected back.\n",
        "3. **Feed-forward network (FFN)** applied per token.\n",
        "4. **Residual connections** + **layer normalization** (training stability).\n",
        "5. **Masking** (for decoder-only / autoregressive models):\n",
        "   - prevents attending to “future” tokens during training.\n",
        "\n",
        "### Encoder-only vs decoder-only vs encoder–decoder\n",
        "- **Encoder-only (BERT-like)**: bidirectional attention; good for understanding tasks (classification, NER).\n",
        "- **Decoder-only (GPT-like)**: causal masked self-attention; good for generation.\n",
        "- **Encoder–decoder (T5-like)**: encoder processes input, decoder generates output with cross-attention.\n",
        "\n",
        "### Complexity note\n",
        "Self-attention is **O(T²)** in sequence length *T*, so very long contexts require specialized techniques (sparse attention, memory, linear attention, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNw0XcV1yHZi"
      },
      "source": [
        "### Mini-example: Multi-Head Attention shapes (Transformer building block)\n",
        "\n",
        "`tf.keras.layers.MultiHeadAttention` implements scaled dot-product attention with multiple heads.\n",
        "\n",
        "In a decoder-only (GPT-like) model, we apply a **causal mask** so position *t* cannot attend to positions > *t*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LlmwXiG4yHZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c735647b-6c29-49e8-b188-3b48dcfa1503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (2, 6, 32)\n",
            "Output shape: (2, 6, 32)\n",
            "Attention scores shape: (2, 4, 6, 6)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "batch = 2\n",
        "T = 6       # sequence length\n",
        "d_model = 32\n",
        "num_heads = 4\n",
        "\n",
        "mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)\n",
        "\n",
        "# Dummy token representations (batch, time, d_model)\n",
        "x = tf.random.normal((batch, T, d_model))\n",
        "\n",
        "# Causal mask: allow attention to self and previous positions only\n",
        "# mask shape: (batch, T, T) with 1 for allowed, 0 for blocked\n",
        "causal = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
        "causal = tf.cast(causal, tf.bool)\n",
        "causal = tf.tile(causal[None, :, :], [batch, 1, 1])\n",
        "\n",
        "y = mha(query=x, value=x, key=x, attention_mask=causal)\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", y.shape)\n",
        "\n",
        "# You can also ask for attention scores (useful for visualization/debugging)\n",
        "y2, scores = mha(query=x, value=x, key=x, attention_mask=causal, return_attention_scores=True)\n",
        "print(\"Attention scores shape:\", scores.shape)  # (batch, heads, T, T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGBv8ppYyHZj"
      },
      "source": [
        "## 11) Summary and quick checks\n",
        "\n",
        "### Key takeaways\n",
        "- NLP pipelines convert text → tokens → ids → embeddings → model.\n",
        "- Language modeling uses the chain rule; training is cross-entropy / NLL.\n",
        "- Full softmax is expensive for large vocabularies; sampled softmax and NCE are common approximations.\n",
        "- Seq2seq models map input sequences to output sequences; attention helps overcome the encoder bottleneck.\n",
        "- Transformers use self-attention + positional encodings and scale well; decoder-only Transformers use causal masking.\n",
        "\n",
        "### Practice questions\n",
        "1. Why does word-level tokenization lead to high OOV rates? How do subword tokenizers fix this?\n",
        "2. Derive perplexity from the average negative log-likelihood.\n",
        "3. What changes between seq2seq training (teacher forcing) and inference?\n",
        "4. Compare encoder-only vs decoder-only Transformers: what tasks are they best at?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}